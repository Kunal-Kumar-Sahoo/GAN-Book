{"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0ec409a904ff4277a9cdfa4dbd1e1ad9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fcf3b539a1d4c8a9abdf9e0342d3d48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_87ee8cbc0fd341d49a4d1833f838ff43","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_56556ce7b15c45c899b5abe08a2aebcf","value":0}},"5505986a639a4ddf8cf1f84ddf03c3fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56556ce7b15c45c899b5abe08a2aebcf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e2244be92554aef91a95fccec927d05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fcded9d560b4d77bba3aab3de696c2c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87ee8cbc0fd341d49a4d1833f838ff43":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8945550af0c74f78b10f7bf9695cbaac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d86788d5b9a456faddc5121325cf647":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ec409a904ff4277a9cdfa4dbd1e1ad9","placeholder":"​","style":"IPY_MODEL_5505986a639a4ddf8cf1f84ddf03c3fb","value":"  0%"}},"affab65279984e08b4e257b9f2d8827e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d86788d5b9a456faddc5121325cf647","IPY_MODEL_1fcf3b539a1d4c8a9abdf9e0342d3d48","IPY_MODEL_d93f56e820b94f3c8eb6b2c239030646"],"layout":"IPY_MODEL_8945550af0c74f78b10f7bf9695cbaac"}},"d93f56e820b94f3c8eb6b2c239030646":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e2244be92554aef91a95fccec927d05","placeholder":"​","style":"IPY_MODEL_7fcded9d560b4d77bba3aab3de696c2c","value":" 0/500 [00:00&lt;?, ?it/s]"}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### RGB v/s $L*a*b$\n\nWhen we load an image, we get a rank-3 tensor with the last axis containing the color data for the image. These data represent color in RGB color space and there are 3 numbers for each pixel indicating how much Red, Green, and Blue the pixel is.\n\nIn $L*a*b$ color space, we have again three dimensions for each pixel. The first dimension (channel) $L$, encodes the lightness of each pixel and when we visualize this channel it appears as black and white image. The $*a$ and $*b$ channels encode how much green-red and yellow-blue each pixel is, respectively.\n\nTo train a model for colorization, we should it give it a grayscale image and hope that it will make it colorful. When using $L*a*b$, we can give the $L$ channel to the model and want it to predict the other channels ($*a$, $*b$) and after its prediction, we concatenate all the channels and we get our colorful image. But if we use RGB image, we have to first convert it to grayscale, feed the grayscale to the model and hope it will predict 3 numbers for which is way more difficult and unstable task due to many more possible combinations of 3 numbers compared to two numbers. If we assume we have 256 choices for each number, predicting the three numbers for each of the pixels is choosing between $256^3$ combinations which is more than 16M choices, but when predicting two numbers we have about 65K choices.\n\n### How to solve the problem?\n\n[*Colorful Image Colorization*](https://arxiv.org/abs/1603.08511) paper approached the problem as a classification task and they have also considered the uncertainty of this problem (e.g- a car in the image can take on many different and valid colors and we cannot be sure about any color about it); however, another paper approached the problem as a regression task.\n\n### Current Implementation\n[*Image-to-Image Translation with Conditional Adversarial Networks*](https://arxiv.org/abs/1611.07004) paper, also known as *pix2pix*, proposed a general solution to many image-to-image tasks in deep learning which one of those was colorization. In this approach two losses are used: *L1 loss*, which makes it a regression task, and an *adversarial (GAN) loss*, which helps to solve the problem in an unsupervised manner.","metadata":{"_uuid":"a5097824-67b4-40fb-824b-b1d485f43d71","_cell_guid":"d927286f-130c-4c69-af8d-2a271cb4fc57","id":"AOYXPwg-rh6I","trusted":true}},{"cell_type":"markdown","source":"### GAN\n\nIn a GAN we have a generator and a discriminator model which learn to solve a problem together. In our setting, the generator model takes a grayscale image ($L$ channel) and produces a 2-channel image ($*a$, $*b$). The discriminator, takes these two produced channels and concatenates them with the input grayscale image and decides whether this new 3-channel image is fake or real.\n\nThe grayscale image which both the generator and discriminator see is the condition that we provide to both the models in our GAN and expect that they take this condition into consideration.\n\nConsider $x$ as the grayscale image, $z$ as the input noise for the generator, and $y$ as the 2-channel output we want from the generator. Also, $G$ is the generator and $D$ is the discriminator. Then the loss for our conditional GAN will be:\n\n$$\\mathcal{L}_{cGAN}(G, D) = \\mathbb{E}_{x, y}[log D(x, y)] + \\mathbb{E}_{x, z}[log(1-D(x, G(x, z))]$$\n\n### Loss function we optimize\n\nThe earlier loss function helps to produce good-looking colorful images that seem real, but to further help the models and introduce some supervision in our task, we combine this loss function with $L1$ loss of the predicted colors compared with the actual colors:\n\n$$\\mathcal{L}_{L1}(G) = \\mathbb{E}_{x, y, z}[||y-G(x, z)||_1]$$\n\nIf we use L1 loss alone, the model still learns to colorize the images but it will be conservative and most of the time uses colors like \"gray\" or \"brown\" because when it doubts which color is the best, it takes the average and uses these colors to reduce the L1 loss as much as possible (similar to blurring effect of L1 or L2 loss in super resolution task). Also, the L1 loss is preferred over L2 loss (or mean squared error) because it reduces that effect of producing gray-ish images. So, our combined loss function will be:\n\n$$G^* = arg \\space \\underset{G}{\\mathrm{min}} \\space \\underset{D}{\\mathrm{max}} \\mathcal{L}_{cGAN}(G, D) + \\lambda \\mathcal{L}_{L1}(G)$$\n\nwhere $\\lambda$ is a coefficient to balance the contribution of the two losses to the final loss (of course the discriminator loss does not involve the L1 loss).","metadata":{"_uuid":"44cabd84-f059-4773-b437-07c30288f343","_cell_guid":"64b5e5e8-df32-4b50-a61b-1d11a46798e4","id":"FMHrIDaCy2Q1","trusted":true}},{"cell_type":"markdown","source":"### Implementing the paper-Baseline","metadata":{"_uuid":"0613b48b-6687-4e53-9a78-15b4c376a505","_cell_guid":"a54b0b99-ec86-47ff-838b-0df0fb446390","id":"c98MZTe_257-","trusted":true}},{"cell_type":"code","source":"%%capture\n!pip install -y scikit-image fastai","metadata":{"_uuid":"6d0c338d-8395-49c5-95ea-640a4ab4953c","_cell_guid":"6b62030c-4fca-4833-b01e-ca4682bc5a44","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:52:32.893927Z","iopub.execute_input":"2024-03-02T05:52:32.894274Z","iopub.status.idle":"2024-03-02T05:52:34.592140Z","shell.execute_reply.started":"2024-03-02T05:52:32.894238Z","shell.execute_reply":"2024-03-02T05:52:34.591032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\nimport time\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom skimage.color import rgb2lab, lab2rgb\n\nimport torch\nfrom torch import nn, optim\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import Dataset, DataLoader\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"_uuid":"b355b0f8-9c8a-449f-8a32-5427b099f287","_cell_guid":"ca5dd903-a9f6-4455-bdcf-bfcaab50a5f8","collapsed":false,"id":"U-pky6FhrfY2","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:52:34.593998Z","iopub.execute_input":"2024-03-02T05:52:34.594296Z","iopub.status.idle":"2024-03-02T05:52:40.761537Z","shell.execute_reply.started":"2024-03-02T05:52:34.594268Z","shell.execute_reply":"2024-03-02T05:52:40.760739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"_uuid":"79c2f531-5620-41f5-8c71-957766db8834","_cell_guid":"8587624c-96a9-4935-b7be-bc4dae7dbb39","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:52:40.762695Z","iopub.execute_input":"2024-03-02T05:52:40.763083Z","iopub.status.idle":"2024-03-02T05:52:40.773081Z","shell.execute_reply.started":"2024-03-02T05:52:40.763058Z","shell.execute_reply":"2024-03-02T05:52:40.772235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.data.external import untar_data, URLs\ncoco_path = untar_data(URLs.COCO_SAMPLE)\ncoco_path = os.path.join(str(coco_path), 'train_sample')","metadata":{"_uuid":"be2c16dc-86dd-4a85-9ed8-a7a1b3f80697","_cell_guid":"4c5b1f89-2c6c-4feb-a87d-a091d4cd64a2","collapsed":false,"id":"L8XZEN2t3jsq","outputId":"d06b9d73-26ee-412f-9468-7ac0711f23ee","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:52:40.776101Z","iopub.execute_input":"2024-03-02T05:52:40.776370Z","iopub.status.idle":"2024-03-02T05:54:07.167383Z","shell.execute_reply.started":"2024-03-02T05:52:40.776328Z","shell.execute_reply":"2024-03-02T05:54:07.166354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths = glob.glob(coco_path + '/*.jpg')\nnp.random.seed(42)\npaths_subset = np.random.choice(paths, 10000, replace=False)\nrand_idxs = np.random.permutation(10000)\ntrain_idxs = rand_idxs[:8000]\nval_idxs = rand_idxs[8000:]\ntrain_paths = paths_subset[train_idxs]\nval_paths = paths_subset[val_idxs]\nprint(len(train_paths), len(val_paths))","metadata":{"_uuid":"7b0a90e0-bbe6-467e-92bd-665cb088f4d0","_cell_guid":"4e2a8d24-f98c-4519-bcf8-a4032b9e5b5e","collapsed":false,"id":"XPwR3MZc39sD","outputId":"eccaad59-9a1a-4bc6-c318-038872fd5c54","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:07.168726Z","iopub.execute_input":"2024-03-02T05:54:07.169598Z","iopub.status.idle":"2024-03-02T05:54:07.267087Z","shell.execute_reply.started":"2024-03-02T05:54:07.169561Z","shell.execute_reply":"2024-03-02T05:54:07.266081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, axes = plt.subplots(4, 4, figsize=(10, 10))\nfor ax, img_path in zip(axes.flatten(), train_paths):\n    ax.imshow(Image.open(img_path))\n    ax.axis('off')","metadata":{"_uuid":"f2344aec-6c07-4dd8-8e5b-14f62b113179","_cell_guid":"e2c8e504-6f2b-40b1-abdf-4f01de8c87ba","collapsed":false,"id":"GBpu0EVg4mSX","outputId":"143044fd-a701-42bc-e41b-97007d93f05c","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:07.268515Z","iopub.execute_input":"2024-03-02T05:54:07.269188Z","iopub.status.idle":"2024-03-02T05:54:11.215502Z","shell.execute_reply.started":"2024-03-02T05:54:07.269153Z","shell.execute_reply":"2024-03-02T05:54:11.214527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIZE = 256\n\nclass ColorizationDataset(Dataset):\n    def __init__(self, paths, split='train'):\n        if split == 'train':\n            self.transforms = transforms.Compose([\n                transforms.Resize((SIZE, SIZE), Image.BICUBIC),\n                transforms.RandomHorizontalFlip()\n            ])\n        elif split == 'val':\n            self.transforms = transforms.Resize((SIZE, SIZE), Image.BICUBIC)\n\n        self.split = split\n        self.size = SIZE\n        self.paths = paths\n\n    def __getitem__(self, index):\n        img = Image.open(self.paths[index]).convert('RGB')\n        img = self.transforms(img)\n        img = np.array(img)\n        img_lab = rgb2lab(img).astype('float32')\n        img_lab = transforms.ToTensor()(img_lab)\n        L = img_lab[[0], ...] / 50. - 1.\n        ab = img_lab[[1, 2], ...] / 110.\n\n        return {'L': L, 'ab': ab}\n\n    def __len__(self):\n        return len(self.paths)\n\ndef make_dataloaders(batch_size=16, n_workers=4, pin_memory=True, **kwargs):\n    dataset = ColorizationDataset(**kwargs)\n    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers, pin_memory=pin_memory)\n\n    return dataloader","metadata":{"_uuid":"697a5c74-9cfd-4926-b24e-8186a63e3057","_cell_guid":"0d1019e2-abdc-42eb-86bf-61d7fa3c5190","collapsed":false,"id":"keiaPnT65A-P","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:11.216900Z","iopub.execute_input":"2024-03-02T05:54:11.217317Z","iopub.status.idle":"2024-03-02T05:54:11.230441Z","shell.execute_reply.started":"2024-03-02T05:54:11.217286Z","shell.execute_reply":"2024-03-02T05:54:11.229521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dl = make_dataloaders(paths=train_paths, split='train')\nval_dl = make_dataloaders(paths=val_paths, split='val')\n\ndata = next(iter(train_dl))\nLs, abs_ = data['L'], data['ab']\nprint(Ls.shape, abs_.shape)\nprint(len(train_dl), len(val_dl))","metadata":{"_uuid":"5699ff29-e20a-4abb-b7ae-57aa7d348e84","_cell_guid":"a6292ea3-ae2c-446a-ac40-29ae74d5a0dc","collapsed":false,"id":"RjkcuO2v6h1K","outputId":"bc837a95-52f0-4ead-83bc-55e332774b70","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:11.231983Z","iopub.execute_input":"2024-03-02T05:54:11.232296Z","iopub.status.idle":"2024-03-02T05:54:13.589271Z","shell.execute_reply.started":"2024-03-02T05:54:11.232252Z","shell.execute_reply":"2024-03-02T05:54:13.588221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNetBlock(nn.Module):\n    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False, innermost=False, outermost=False):\n        super().__init__()\n        self.outermost = outermost\n        if input_c is None:\n            input_c = nf\n        downconv = nn.Conv2d(input_c, ni, kernel_size=4, stride=2, padding=1, bias=False)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = nn.BatchNorm2d(ni)\n        uprelu = nn.ReLU(True)\n        upnorm = nn.BatchNorm2d(nf)\n\n        if outermost:\n            upconv = nn.ConvTranspose2d(ni*2, nf, kernel_size=4, stride=2, padding=1)\n            down = [downconv]\n            up = [uprelu, upconv, nn.Tanh()]\n            model = down + [submodule] + up\n\n        elif innermost:\n            upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4, stride=2, padding=1, bias=False)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n\n        else:\n            upconv = nn.ConvTranspose2d(ni*2, nf, kernel_size=4, stride=2, padding=1, bias=False)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n            if dropout:\n                up += [nn.Dropout(0.5)]\n            model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            return torch.cat([x, self.model(x)], 1)","metadata":{"_uuid":"7af1c120-c089-473a-8afd-32233226dccb","_cell_guid":"ab36caa0-e688-4f16-9340-46873ff83163","collapsed":false,"id":"o2U_o_Dg62re","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:13.590954Z","iopub.execute_input":"2024-03-02T05:54:13.591360Z","iopub.status.idle":"2024-03-02T05:54:13.604135Z","shell.execute_reply.started":"2024-03-02T05:54:13.591307Z","shell.execute_reply":"2024-03-02T05:54:13.603091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n        super().__init__()\n        unet_block = UNetBlock(num_filters*8, num_filters*8, innermost=True)\n        for _ in range(n_down-5):\n            unet_block = UNetBlock(num_filters*8, num_filters*8, submodule=unet_block, dropout=True)\n        out_filters = num_filters*8\n        for _ in range(3):\n            unet_block = UNetBlock(out_filters//2, out_filters, submodule=unet_block)\n            out_filters //= 2\n        self.model = UNetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"_uuid":"4ecff2f1-75b0-4834-b73a-b714b631fcaa","_cell_guid":"69a9ca7e-aa26-4768-9335-bdeebd70901d","collapsed":false,"id":"6r0bgJYX9L1F","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:13.608414Z","iopub.execute_input":"2024-03-02T05:54:13.608770Z","iopub.status.idle":"2024-03-02T05:54:13.621960Z","shell.execute_reply.started":"2024-03-02T05:54:13.608740Z","shell.execute_reply":"2024-03-02T05:54:13.621120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In a vanilla discriminator, the model outputs a scalar quantity which represents how much the model thinks the input is real or fake. In a patch discriminator, the model outputs one number for every patch of the input image and for each of them decides whether it is fake or not seperately. Using such a model for the task of colorization seems reasonable to me because the local changes that the model needs to make are really important and maybe deciding on the whole image as in vanilla discriminator cannot take care of the subtleties of this task.","metadata":{"_uuid":"fa354d08-c5d9-44b8-ad7b-0336b34cef26","_cell_guid":"afb04e6b-fd8a-497d-b71a-cf9b3d84a7d1","id":"wMTUle3U_mS1","trusted":true}},{"cell_type":"code","source":"class PatchDiscriminator(nn.Module):\n    def __init__(self, input_c, num_filters=64, n_down=3):\n        super().__init__()\n        model = [self.get_layers(input_c, num_filters, norm=False)]\n        model += [self.get_layers(num_filters*2**i, num_filters*2**(i+1), s=1 if i == (n_down-1) else 2) for i in range(n_down)]\n        model += [self.get_layers(num_filters*2**n_down, 1, s=1, norm=False, act=False)]\n        self.model = nn.Sequential(*model)\n\n    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True):\n        layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]\n        if norm:\n            layers += [nn.BatchNorm2d(nf)]\n        if act:\n            layers += [nn.LeakyReLU(0.2, True)]\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"_uuid":"10890781-6414-408e-892e-3db47395f73f","_cell_guid":"8f13e423-89f5-4d28-a4af-7cce60928c19","collapsed":false,"id":"iIW4TfYt-F8k","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:13.623065Z","iopub.execute_input":"2024-03-02T05:54:13.623365Z","iopub.status.idle":"2024-03-02T05:54:13.637531Z","shell.execute_reply.started":"2024-03-02T05:54:13.623318Z","shell.execute_reply":"2024-03-02T05:54:13.636546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PatchDiscriminator(3)","metadata":{"_uuid":"974a02f9-0d19-45aa-930c-1ac0c8e8b0ee","_cell_guid":"2dcbeb0c-57c2-401c-b561-37484bae7970","collapsed":false,"id":"d8-01WSS_Rvc","outputId":"0493c74f-c33a-4491-d596-cbc96ebdbe49","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:13.639287Z","iopub.execute_input":"2024-03-02T05:54:13.639644Z","iopub.status.idle":"2024-03-02T05:54:13.688795Z","shell.execute_reply.started":"2024-03-02T05:54:13.639612Z","shell.execute_reply":"2024-03-02T05:54:13.687849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator = PatchDiscriminator(3)\ndummy_input = torch.randn(16, 3, 256, 256)\nout = discriminator(dummy_input)\nout.shape","metadata":{"_uuid":"10bb5ce7-cdfc-4737-88b2-749a0c5ed71d","_cell_guid":"299a8d7d-4ca9-4b3d-bc98-2856cc4c105d","collapsed":false,"id":"SCwTqDMb_VUm","outputId":"88381a13-7e50-458e-83f5-9e78cecf38b6","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:13.690006Z","iopub.execute_input":"2024-03-02T05:54:13.690296Z","iopub.status.idle":"2024-03-02T05:54:15.143206Z","shell.execute_reply.started":"2024-03-02T05:54:13.690272Z","shell.execute_reply":"2024-03-02T05:54:15.142217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GANLoss(nn.Module):\n    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n        super().__init__()\n        self.register_buffer('real_label', torch.tensor(real_label))\n        self.register_buffer('fake_label', torch.tensor(fake_label))\n\n        if gan_mode == 'vanilla':\n            self.loss = nn.BCEWithLogitsLoss()\n        elif gan_mode == 'lsgan':\n            self.loss = nn.MSELoss()\n\n    def get_labels(self, preds, target_is_real):\n        if target_is_real:\n            labels = self.real_label\n        else:\n            labels = self.fake_label\n        return labels.expand_as(preds)\n\n    def __call__(self, preds, target_is_real):\n        labels = self.get_labels(preds, target_is_real)\n        loss = self.loss(preds, labels)\n        return loss","metadata":{"_uuid":"af209f18-efd0-46a7-9c09-6eb7531651c7","_cell_guid":"3e26321b-2d97-4785-b187-2eb283dba06f","collapsed":false,"id":"SEB1W2uuAn8S","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:15.144743Z","iopub.execute_input":"2024-03-02T05:54:15.145155Z","iopub.status.idle":"2024-03-02T05:54:15.155659Z","shell.execute_reply.started":"2024-03-02T05:54:15.145119Z","shell.execute_reply":"2024-03-02T05:54:15.152613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_weights(net, init='norm', gain=0.02):\n\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and 'Conv' in classname:\n            if init == 'norm':\n                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n            elif init == 'xavier':\n                nn.init.xavier_normal(m.weight.data, gain=gain)\n            elif init == 'kaiming':\n                nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n\n            if hasattr(m, 'bias') and m.bias is not None:\n                nn.init.constant_(m.bias.data, 0.0)\n\n        elif 'BatchNorm2d' in classname:\n            nn.init.normal_(m.weight.data, 1., gain)\n            nn.init.constant_(m.bias.data, 0.)\n\n    net.apply(init_func)\n    print(f'Model initialized with {init} initialization')\n    return net\n\ndef init_model(model, device):\n    model = model.to(device)\n    model = init_weights(model)\n    return model","metadata":{"_uuid":"9c24ed44-330e-407a-8da3-39c4ee5eef1b","_cell_guid":"559a49a7-bef8-4c1c-ab9e-22d13f521bc6","collapsed":false,"id":"oDduaFtDBeyf","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:15.157129Z","iopub.execute_input":"2024-03-02T05:54:15.157459Z","iopub.status.idle":"2024-03-02T05:54:15.205063Z","shell.execute_reply.started":"2024-03-02T05:54:15.157434Z","shell.execute_reply":"2024-03-02T05:54:15.204041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MainModel(nn.Module):\n    def __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4, beta1=0.5, beta2=0.999, lambda_L1=100.):\n        super().__init__()\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.lambda_L1 = lambda_L1\n\n        if net_G is None:\n            self.net_G = init_model(UNet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n        else:\n            self.net_G = net_G.to(self.device)\n        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n        self.L1criterion = nn.L1Loss()\n        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n\n    def set_requires_grad(self, model, requires_grad=True):\n        for p in model.parameters():\n            p.requires_grad = requires_grad\n\n    def setup_input(self, data):\n        self.L = data['L'].to(self.device)\n        self.ab = data['ab'].to(self.device)\n\n    def forward(self):\n        self.fake_color = self.net_G(self.L)\n\n    def backward_D(self):\n        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n        fake_preds = self.net_D(fake_image.detach())\n        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n        real_image = torch.cat([self.L, self.ab], dim=1)\n        real_preds = self.net_D(real_image)\n        self.loss_D_real = self.GANcriterion(real_preds, True)\n        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n        self.loss_D.backward()\n\n    def backward_G(self):\n        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n        fake_preds = self.net_D(fake_image)\n        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n        return self.loss_G.backward()\n\n    def optimize(self):\n        self.forward()\n        self.net_D.train()\n        self.set_requires_grad(self.net_D, True)\n        self.opt_D.zero_grad()\n        self.backward_D()\n        self.opt_D.step()\n\n        self.net_G.train()\n        self.set_requires_grad(self.net_D, False)\n        self.opt_G.zero_grad()\n        self.backward_G()\n        self.opt_G.step()","metadata":{"_uuid":"e5123a5a-1e47-4ded-8705-c3072a7ab950","_cell_guid":"f0564fda-76f8-4388-97b1-a0102a1b3140","collapsed":false,"id":"wCzESX1fCmWL","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:15.206323Z","iopub.execute_input":"2024-03-02T05:54:15.206640Z","iopub.status.idle":"2024-03-02T05:54:15.224602Z","shell.execute_reply.started":"2024-03-02T05:54:15.206616Z","shell.execute_reply":"2024-03-02T05:54:15.223595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.count, self.avg, self.sum = [0.] * 3\n\n    def update(self, val, count=1):\n        self.count += count\n        self.sum += count * val\n        self.avg = self.sum / self.count\n\ndef create_loss_meters():\n    loss_D_fake = AverageMeter()\n    loss_D_real = AverageMeter()\n    loss_D = AverageMeter()\n    loss_G_GAN = AverageMeter()\n    loss_G_L1 = AverageMeter()\n    loss_G = AverageMeter()\n\n    return {\n        'loss_D_fake': loss_D_fake,\n        'loss_D_real': loss_D_real,\n        'loss_D': loss_D,\n        'loss_G_GAN': loss_G_GAN,\n        'loss_G_L1': loss_G_L1,\n        'loss_G': loss_G\n    }\n\ndef update_losses(model, loss_meter_dict, count):\n    for loss_name, loss_meter in loss_meter_dict.items():\n        loss = getattr(model, loss_name)\n        loss_meter.update(loss.item(), count=count)\n\ndef lab_to_rgb(L, ab):\n    L = (L + 1.) * 50.\n    ab *= 110.\n    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n    rgb_imgs = []\n    for img in Lab:\n        img_rgb = lab2rgb(img)\n        rgb_imgs.append(img_rgb)\n    return np.stack(rgb_imgs, axis=0)\n\ndef visualize(model, data, save=True):\n    model.net_G.eval()\n    with torch.no_grad():\n        model.setup_input(data)\n        model.forward()\n    model.net_G.train()\n    fake_color = model.fake_color.detach()\n    real_color = model.ab\n    L = model.L\n    fake_imgs = lab_to_rgb(L, fake_color)\n    real_imgs = lab_to_rgb(L, real_color)\n    fig = plt.figure(figsize=(15, 8))\n    for i in range(5):\n        ax = plt.subplot(3, 5, i+1)\n        ax.imshow(L[i][0].cpu(), cmap='gray')\n        ax.axis('off')\n        ax = plt.subplot(3, 5, i+1+5)\n        ax.imshow(fake_imgs[i])\n        ax.axis('off')\n        ax = plt.subplot(3, 5, i+1+10)\n        ax.imshow(real_imgs[i])\n        ax.axis('off')\n    plt.show()\n    if save:\n        fig.savefig(f'colorization_{time.time()}.png')\n\ndef log_results(loss_meter_dict):\n    for loss_name, loss_meter in loss_meter_dict.items():\n        print(f'{loss_name}: {loss_meter.avg:.5f}')","metadata":{"_uuid":"ea885bec-4ad8-4929-88ce-e2403d6010ff","_cell_guid":"ac97718c-0ab2-4c8d-ba0d-f55d7a67cfd2","collapsed":false,"id":"a-6xMFDtFoSs","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:15.225857Z","iopub.execute_input":"2024-03-02T05:54:15.226178Z","iopub.status.idle":"2024-03-02T05:54:15.242437Z","shell.execute_reply.started":"2024-03-02T05:54:15.226150Z","shell.execute_reply":"2024-03-02T05:54:15.241399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_dl, epochs, display_every=200):\n    data = next(iter(val_dl))\n    for e in range(epochs):\n        loss_meter_dict = create_loss_meters()\n        i = 0\n        for data in tqdm(train_dl):\n            model.setup_input(data)\n            model.optimize()\n            update_losses(model, loss_meter_dict, count=data['L'].size(0))\n            i += 1\n            if i % display_every == 0:\n                print(f'\\nEpoch {e+1}/{epochs}')\n                print(f'Iteration {i}/{len(train_dl)}')\n                log_results(loss_meter_dict)\n                visualize(model, data, save=True)\n\nmodel = MainModel()\ntrain_model(model, train_dl, 20)","metadata":{"_uuid":"ca9e5932-eca2-4e2a-9f7f-4d36749575fa","_cell_guid":"ddc712a5-9e22-430f-8726-0118eed17b72","collapsed":false,"id":"Wg6Ha0S5I40f","outputId":"f1ce1398-468d-4cb6-b799-059f3338beee","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T05:54:15.243685Z","iopub.execute_input":"2024-03-02T05:54:15.243998Z","iopub.status.idle":"2024-03-02T06:42:16.797129Z","shell.execute_reply.started":"2024-03-02T05:54:15.243974Z","shell.execute_reply":"2024-03-02T06:42:16.796159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### New Strategy:\n\nInspired by an idea in Super Resolution paper, we will pretrain the generator separately in a supervised and deterministic manner to avoid the problem of \"the blind leading the blind\" in the GAN where neither the generator nor discriminator knows anything about the task at the beginning of training.\n\nPretraining can be done in 2 stages:\n1. The backbone of the generator (the down sampling path) is a pretrained model for classification (on ImageNet)\n2. The whole generator will be pretrained on the task of colorization with L1 loss.\n\nWe will use a pretrained ResNet18 as the backbone of the U-Net and to accomplish the second stage of pretraining, we will train the U-Net on our training set with only L1 loss. Then we will move to the combined adversarial and L1 loss.","metadata":{"_uuid":"db57bea1-4dbb-4558-ac8e-ef3def379b2b","_cell_guid":"8586a4a4-3b35-4298-8706-bea5b6485968","trusted":true}},{"cell_type":"code","source":"from fastai.vision.learner import create_body\nfrom torchvision.models import resnet18\nfrom fastai.vision.models.unet import DynamicUnet","metadata":{"_uuid":"85ef7d69-16ae-4fe0-a67d-28929c14fdb4","_cell_guid":"4d08aacd-8b6e-476c-94c4-14c43317453d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T07:16:04.703002Z","iopub.execute_input":"2024-03-02T07:16:04.703390Z","iopub.status.idle":"2024-03-02T07:16:04.708284Z","shell.execute_reply.started":"2024-03-02T07:16:04.703360Z","shell.execute_reply":"2024-03-02T07:16:04.707263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_res_unet(n_input=1, n_output=2, size=256):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    body = create_body(resnet18(), pretrained=True, n_in=n_input, cut=-2)\n    net_G = DynamicUnet(body, n_output, (size, size)).to(device)\n    return net_G","metadata":{"_uuid":"0794b956-1ac4-4c21-8a88-fb1073506815","_cell_guid":"ed3bf8df-70b9-4add-a4f3-a9c6261103cc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T07:18:21.446350Z","iopub.execute_input":"2024-03-02T07:18:21.447247Z","iopub.status.idle":"2024-03-02T07:18:21.454499Z","shell.execute_reply.started":"2024-03-02T07:18:21.447196Z","shell.execute_reply":"2024-03-02T07:18:21.453274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pretrained_generator(net_G, train_dl, opt, criterion, epochs):\n    for e in range(epochs):\n        loss_meter = AverageMeter()\n        for data in tqdm(train_dl):\n            L, ab = data['L'].to(device), data['ab'].to(device)\n            preds = net_G(L)\n            loss = criterion(preds, ab)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            \n            loss_meter.update(loss.item(), L.size(0))\n            \n        print(f'Epoch {e+1}/{epochs}')\n        print(f'L1 loss: {loss_meter.avg:.5f}')\n        \nnet_G = build_res_unet(n_input=1, n_output=2, size=256)\nopt = optim.Adam(net_G.parameters(), lr=1e-4)\ncriterion = nn.L1Loss()\npretrained_generator(net_G, train_dl, opt, criterion, 20)\ntorch.save(net_G.state_dict(), 'res18-unet.pt')","metadata":{"_uuid":"218083c1-b8a1-4195-a3ba-965782c0c7a7","_cell_guid":"3866d0a4-abbe-4e19-a550-97bf81caa8a3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T07:18:43.791874Z","iopub.execute_input":"2024-03-02T07:18:43.792247Z","iopub.status.idle":"2024-03-02T08:13:03.759162Z","shell.execute_reply.started":"2024-03-02T07:18:43.792216Z","shell.execute_reply":"2024-03-02T08:13:03.758033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net_G = build_res_unet(n_input=1, n_output=2, size=256)\nnet_G.load_state_dict(torch.load('res18-unet.pt', map_location=device))\nmodel = MainModel(net_G=net_G)\ntrain_model(model, train_dl, 20)","metadata":{"_uuid":"de14d2b1-185d-4294-9bab-b7d2810b34de","_cell_guid":"ffcb6f4a-5851-4039-86ff-9004dd37a97b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-02T08:13:03.761521Z","iopub.execute_input":"2024-03-02T08:13:03.761840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comparing the results of the pretrained U-Net and without adversarial training\n\nU-Net we built with the ResNet18 backbone is performing well in colorizing images after pretraining with L1 loss only (a step before the final adversarial training). But, the model is still conservative and encourages using gray-ish colors when it is not sure about what the object is or what color it should be. However, it performs really well for common scenes in the images like sky, tree, grass etc.","metadata":{"_uuid":"483a49fa-3f4a-4d7d-940a-465517937b49","_cell_guid":"b2eb573c-b18d-4f52-89c8-ec61661e83c1","trusted":true}}]}